{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " Helmholtz_Machine"
      ],
      "metadata": {
        "id": "j3cXKu3BFac0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "7QzYa4sEeBE5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYrhAXiYeBMZ",
        "outputId": "a105d42e-9877-4c40-c7c9-1ac20e754430"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7a96ac3313d0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 784  # 28x28 images\n",
        "hidden_size = 200  # Latent variable size\n",
        "num_classes = 10  # MNIST digits\n",
        "batch_size = 100\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "dropout_prob = 0.5  # Dropout probability\n"
      ],
      "metadata": {
        "id": "R41mg_uoeBTt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "vuf6r8bkeBWh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RecognitionNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes, dropout_prob=0.5):\n",
        "        super(RecognitionNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc_class = nn.Linear(hidden_size, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, input_size)\n",
        "        h = self.dropout(self.relu(self.fc1(x)))\n",
        "        z_probs = torch.sigmoid(self.fc2(h))\n",
        "        if self.training:\n",
        "            z = torch.bernoulli(z_probs)\n",
        "        else:\n",
        "            z = z_probs\n",
        "        class_logits = self.fc_class(h)\n",
        "        return z, z_probs, class_logits"
      ],
      "metadata": {
        "id": "QOpIbmvKgm1H"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Top - Down Approach\n",
        "class GenerativeNetwork(nn.Module):\n",
        "    def __init__(self, hidden_size, input_size, dropout_prob=0.5):\n",
        "        super(GenerativeNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, input_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "    def forward(self, z):\n",
        "        h = self.dropout(self.relu(self.fc1(z)))\n",
        "        x_recon_probs = torch.sigmoid(self.fc2(h))\n",
        "        return x_recon_probs\n",
        "class HelmholtzMachine(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes, dropout_prob=0.5):\n",
        "        super(HelmholtzMachine, self).__init__()\n",
        "        self.recognition = RecognitionNetwork(input_size, hidden_size, num_classes, dropout_prob)\n",
        "        self.generative = GenerativeNetwork(hidden_size, input_size, dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z, z_probs, class_logits = self.recognition(x)\n",
        "        x_recon = self.generative(z)\n",
        "        return x_recon, z, z_probs, class_logits\n",
        "\n",
        "    def generate(self, batch_size):\n",
        "        \"\"\"Generate samples by sampling from the prior and running the generative model\"\"\"\n",
        "        device = next(self.parameters()).device\n",
        "        # Sample from prior\n",
        "        z_dream = torch.bernoulli(torch.ones(batch_size, hidden_size) * 0.5).to(device)\n",
        "        x_dream = self.generative(z_dream)\n",
        "        return x_dream, z_dream"
      ],
      "metadata": {
        "id": "wzRTqxMsgm4P"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    wake_loss_total = 0\n",
        "    sleep_loss_total = 0\n",
        "    class_loss_total = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        x_recon, z, z_probs, class_logits = model(data)\n",
        "        recon_loss = nn.BCELoss()(x_recon, data.view(-1, input_size))\n",
        "        class_loss = criterion(class_logits, target)\n",
        "        wake_loss = recon_loss + class_loss\n",
        "        wake_loss.backward(retain_graph=True)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        x_dream, z_dream = model.generate(data.size(0))\n",
        "        _, _, z_dream_recon_probs, _ = model(x_dream.detach())\n",
        "        sleep_loss = nn.BCELoss()(z_dream_recon_probs, z_dream)\n",
        "        sleep_loss.backward()\n",
        "        optimizer.step()\n",
        "        wake_loss_total += wake_loss.item()\n",
        "        sleep_loss_total += sleep_loss.item()\n",
        "        class_loss_total += class_loss.item()\n",
        "        _, predicted = torch.max(class_logits.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "    accuracy = 100. * correct / total\n",
        "    return wake_loss_total / len(train_loader), sleep_loss_total / len(train_loader), class_loss_total / len(train_loader), accuracy\n"
      ],
      "metadata": {
        "id": "aYhsFSNVgm61"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            _, _, _, class_logits = model(data)\n",
        "            test_loss += criterion(class_logits, target).item()\n",
        "            _, predicted = torch.max(class_logits.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "    test_loss /= len(test_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    return test_loss, accuracy"
      ],
      "metadata": {
        "id": "phcJTY2tgm9c"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = HelmholtzMachine(input_size, hidden_size, num_classes, dropout_prob).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for epoch in range(num_epochs):\n",
        "        wake_loss, sleep_loss, class_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
        "        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'  Wake Loss: {wake_loss:.4f}, Sleep Loss: {sleep_loss:.4f}, Class Loss: {class_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "        print(f'  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKCAJqTMgnAD",
        "outputId": "6d2b04f3-c19b-4ba6-edcc-c6e420ea0d43"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10:\n",
            "  Wake Loss: 0.6618, Sleep Loss: 0.6931, Class Loss: 0.4095, Train Acc: 88.66%\n",
            "  Test Loss: 0.1821, Test Acc: 94.63%\n",
            "Epoch 2/10:\n",
            "  Wake Loss: 0.4212, Sleep Loss: 0.6861, Class Loss: 0.2047, Train Acc: 94.00%\n",
            "  Test Loss: 0.1313, Test Acc: 96.16%\n",
            "Epoch 3/10:\n",
            "  Wake Loss: 0.3712, Sleep Loss: 0.6859, Class Loss: 0.1631, Train Acc: 95.25%\n",
            "  Test Loss: 0.1089, Test Acc: 96.72%\n",
            "Epoch 4/10:\n",
            "  Wake Loss: 0.3420, Sleep Loss: 0.6857, Class Loss: 0.1380, Train Acc: 95.91%\n",
            "  Test Loss: 0.0930, Test Acc: 97.33%\n",
            "Epoch 5/10:\n",
            "  Wake Loss: 0.3242, Sleep Loss: 0.6852, Class Loss: 0.1230, Train Acc: 96.28%\n",
            "  Test Loss: 0.0851, Test Acc: 97.30%\n",
            "Epoch 6/10:\n",
            "  Wake Loss: 0.3094, Sleep Loss: 0.6850, Class Loss: 0.1107, Train Acc: 96.61%\n",
            "  Test Loss: 0.0801, Test Acc: 97.49%\n",
            "Epoch 7/10:\n",
            "  Wake Loss: 0.2989, Sleep Loss: 0.6845, Class Loss: 0.1026, Train Acc: 96.90%\n",
            "  Test Loss: 0.0778, Test Acc: 97.70%\n",
            "Epoch 8/10:\n",
            "  Wake Loss: 0.2902, Sleep Loss: 0.6843, Class Loss: 0.0956, Train Acc: 97.02%\n",
            "  Test Loss: 0.0776, Test Acc: 97.71%\n",
            "Epoch 9/10:\n",
            "  Wake Loss: 0.2811, Sleep Loss: 0.6842, Class Loss: 0.0884, Train Acc: 97.20%\n",
            "  Test Loss: 0.0743, Test Acc: 97.82%\n",
            "Epoch 10/10:\n",
            "  Wake Loss: 0.2757, Sleep Loss: 0.6842, Class Loss: 0.0840, Train Acc: 97.29%\n",
            "  Test Loss: 0.0770, Test Acc: 97.74%\n"
          ]
        }
      ]
    }
  ]
}